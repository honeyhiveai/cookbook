{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kmbidkvbkp-"
      },
      "outputs": [],
      "source": [
        "!pip install datasets litellm honeyhive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US93c1c_bw_1"
      },
      "outputs": [],
      "source": [
        "from honeyhive import HoneyHiveTracer\n",
        "\n",
        "# Add this code at the beginning of your AI pipeline code\n",
        "HoneyHiveTracer.init(\n",
        "    api_key=\"dXV3cXpoZmFwb3NsY3N4N3lidmE2aQ==\",\n",
        "    project=\"text2sql-evals\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLCozlOZcLLl"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"suzyanil/nba-data\")[\"train\"]\n",
        "\n",
        "conn = duckdb.connect(database=\":memory:\", read_only=False)\n",
        "conn.register(\"nba\", data.to_pandas())\n",
        "\n",
        "conn.query(\"SELECT * FROM nba LIMIT 5\").to_df().to_dict(orient=\"records\")[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eAzYjyuEs2-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import logging\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from litellm import completion\n",
        "from honeyhive import evaluate, evaluator\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"your gemini key\"\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"your anthropic key\"\n",
        "\n",
        "# Load NBA dataset\n",
        "data = load_dataset(\"suzyanil/nba-data\")[\"train\"]\n",
        "conn = duckdb.connect(database=\":memory:\", read_only=False)\n",
        "conn.register(\"nba\", data.to_pandas())\n",
        "logger.info(\"NBA dataset loaded and registered with DuckDB\")\n",
        "\n",
        "# Get column information\n",
        "columns = conn.query(\"DESCRIBE nba\").to_df().to_dict(orient=\"records\")\n",
        "samples = conn.query(\"SELECT * FROM nba LIMIT 1\").to_df().to_dict(orient=\"records\")[0]\n",
        "sample_rows = \"\\n\".join(\n",
        "    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n",
        "    for column in columns\n",
        ")\n",
        "\n",
        "# Define system prompt with examples\n",
        "system_prompt = (\n",
        "    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\\n\"\n",
        "    \"Column | Type | Example\\n\"\n",
        "    \"-------|------|--------\\n\"\n",
        "    f\"{sample_rows}\\n\"\n",
        "    \"\\n\"\n",
        "    \"Write a DuckDB SQL query corresponding to the user's request. \"\n",
        "    \"Return just the query text, with no formatting (backticks, markdown, etc.).\"\n",
        ")\n",
        "\n",
        "# Helper function to clean SQL query from markdown formatting\n",
        "def clean_sql_query(query):\n",
        "    \"\"\"Remove markdown formatting from SQL query\"\"\"\n",
        "    # Remove ```sql and ``` markers\n",
        "    query = re.sub(r'```sql\\s*', '', query)\n",
        "    query = re.sub(r'```\\s*', '', query)\n",
        "    return query.strip()\n",
        "\n",
        "# Generate SQL query with OpenAI\n",
        "def generate_query_openai(question):\n",
        "    \"\"\"Generate SQL query from natural language question using OpenAI\"\"\"\n",
        "    logger.info(f\"Generating SQL query with OpenAI for question: {question}\")\n",
        "    response = completion(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt,\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": question,\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "    query = response.choices[0].message.content\n",
        "    query = clean_sql_query(query)\n",
        "    logger.info(f\"Generated query with OpenAI: {query}\")\n",
        "    return query\n",
        "\n",
        "# Generate SQL query with Gemini\n",
        "def generate_query_gemini(question):\n",
        "    \"\"\"Generate SQL query from natural language question using Gemini\"\"\"\n",
        "    logger.info(f\"Generating SQL query with Gemini for question: {question}\")\n",
        "    response = completion(\n",
        "        model=\"gemini/gemini-2.0-flash\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt + \"\\n\\nIMPORTANT: Do not include any markdown formatting, code blocks, or backticks in your response. Just provide the raw SQL query.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": question,\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "    query = response.choices[0].message.content\n",
        "    query = clean_sql_query(query)\n",
        "    logger.info(f\"Generated query with Gemini: {query}\")\n",
        "    return query\n",
        "\n",
        "# Generate SQL query with Anthropic\n",
        "def generate_query_anthropic(question):\n",
        "    \"\"\"Generate SQL query from natural language question using Anthropic\"\"\"\n",
        "    logger.info(f\"Generating SQL query with Anthropic for question: {question}\")\n",
        "    response = completion(\n",
        "        model=\"claude-3-7-sonnet-20250219\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt + \"\\n\\nIMPORTANT: Do not include any markdown formatting, code blocks, or backticks in your response. Just provide the raw SQL query.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": question,\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "    query = response.choices[0].message.content\n",
        "    query = clean_sql_query(query)\n",
        "    logger.info(f\"Generated query with Anthropic: {query}\")\n",
        "    return query\n",
        "\n",
        "# Execute SQL query\n",
        "def execute_query(query):\n",
        "    \"\"\"Execute SQL query and return results or error\"\"\"\n",
        "    logger.info(f\"Executing query: {query}\")\n",
        "    try:\n",
        "        results = conn.query(query).fetchdf().to_dict(orient=\"records\")\n",
        "        logger.info(f\"Query executed successfully. Result count: {len(results)}\")\n",
        "        return {\"results\": results, \"error\": None}\n",
        "    except duckdb.Error as e:\n",
        "        error_msg = str(e)\n",
        "        logger.error(f\"Query execution failed: {error_msg}\")\n",
        "        return {\"results\": None, \"error\": error_msg}\n",
        "\n",
        "# Main text2sql function for OpenAI evaluation\n",
        "def text2sql_openai(inputs, ground_truths=None):\n",
        "    \"\"\"\n",
        "    Main function that processes a question and returns SQL results using OpenAI.\n",
        "\n",
        "    Parameters:\n",
        "      - inputs: Dict with a 'question' key\n",
        "      - ground_truths: Optional ground truth data (not used in this example)\n",
        "    \"\"\"\n",
        "    logger.info(f\"text2sql_openai called with inputs: {inputs}, type: {type(inputs)}\")\n",
        "\n",
        "    # Extract the question from inputs\n",
        "    if isinstance(inputs, dict) and 'question' in inputs:\n",
        "        question = inputs['question']\n",
        "        logger.info(f\"Processing question from inputs dict with OpenAI: {question}\")\n",
        "    else:\n",
        "        error_msg = f\"Invalid input format: {inputs}, type: {type(inputs)}\"\n",
        "        logger.error(error_msg)\n",
        "        return {\n",
        "            \"model\": \"gpt-4o\",\n",
        "            \"query\": \"\",\n",
        "            \"results\": [],\n",
        "            \"error\": \"Invalid input format - expected dict with 'question' key\"\n",
        "        }\n",
        "\n",
        "    # Generate SQL query\n",
        "    query = generate_query_openai(question)\n",
        "\n",
        "    # Execute query\n",
        "    execution_result = execute_query(query)\n",
        "\n",
        "    # Combine results\n",
        "    result = {\n",
        "        \"model\": \"gpt-4o\",\n",
        "        \"query\": query,\n",
        "        \"results\": execution_result[\"results\"],\n",
        "        \"error\": execution_result[\"error\"]\n",
        "    }\n",
        "\n",
        "    logger.info(f\"text2sql_openai completed. Result: {json.dumps(result, default=str)[:200]}...\")\n",
        "    return result\n",
        "\n",
        "# Main text2sql function for Gemini evaluation\n",
        "def text2sql_gemini(inputs, ground_truths=None):\n",
        "    \"\"\"\n",
        "    Main function that processes a question and returns SQL results using Gemini.\n",
        "\n",
        "    Parameters:\n",
        "      - inputs: Dict with a 'question' key\n",
        "      - ground_truths: Optional ground truth data (not used in this example)\n",
        "    \"\"\"\n",
        "    logger.info(f\"text2sql_gemini called with inputs: {inputs}, type: {type(inputs)}\")\n",
        "\n",
        "    # Extract the question from inputs\n",
        "    if isinstance(inputs, dict) and 'question' in inputs:\n",
        "        question = inputs['question']\n",
        "        logger.info(f\"Processing question from inputs dict with Gemini: {question}\")\n",
        "    else:\n",
        "        error_msg = f\"Invalid input format: {inputs}, type: {type(inputs)}\"\n",
        "        logger.error(error_msg)\n",
        "        return {\n",
        "            \"model\": \"gemini-2.0-flash\",\n",
        "            \"query\": \"\",\n",
        "            \"results\": [],\n",
        "            \"error\": \"Invalid input format - expected dict with 'question' key\"\n",
        "        }\n",
        "\n",
        "    # Generate SQL query\n",
        "    query = generate_query_gemini(question)\n",
        "\n",
        "    # Execute query\n",
        "    execution_result = execute_query(query)\n",
        "\n",
        "    # Combine results\n",
        "    result = {\n",
        "        \"model\": \"gemini-2.0-flash\",\n",
        "        \"query\": query,\n",
        "        \"results\": execution_result[\"results\"],\n",
        "        \"error\": execution_result[\"error\"]\n",
        "    }\n",
        "\n",
        "    logger.info(f\"text2sql_gemini completed. Result: {json.dumps(result, default=str)[:200]}...\")\n",
        "    return result\n",
        "\n",
        "# Main text2sql function for Anthropic evaluation\n",
        "def text2sql_anthropic(inputs, ground_truths=None):\n",
        "    \"\"\"\n",
        "    Main function that processes a question and returns SQL results using Anthropic.\n",
        "\n",
        "    Parameters:\n",
        "      - inputs: Dict with a 'question' key\n",
        "      - ground_truths: Optional ground truth data (not used in this example)\n",
        "    \"\"\"\n",
        "    logger.info(f\"text2sql_anthropic called with inputs: {inputs}, type: {type(inputs)}\")\n",
        "\n",
        "    # Extract the question from inputs\n",
        "    if isinstance(inputs, dict) and 'question' in inputs:\n",
        "        question = inputs['question']\n",
        "        logger.info(f\"Processing question from inputs dict with Anthropic: {question}\")\n",
        "    else:\n",
        "        error_msg = f\"Invalid input format: {inputs}, type: {type(inputs)}\"\n",
        "        logger.error(error_msg)\n",
        "        return {\n",
        "            \"model\": \"claude-3-7-sonnet\",\n",
        "            \"query\": \"\",\n",
        "            \"results\": [],\n",
        "            \"error\": \"Invalid input format - expected dict with 'question' key\"\n",
        "        }\n",
        "\n",
        "    # Generate SQL query\n",
        "    query = generate_query_anthropic(question)\n",
        "\n",
        "    # Execute query\n",
        "    execution_result = execute_query(query)\n",
        "\n",
        "    # Combine results\n",
        "    result = {\n",
        "        \"model\": \"claude-3-7-sonnet\",\n",
        "        \"query\": query,\n",
        "        \"results\": execution_result[\"results\"],\n",
        "        \"error\": execution_result[\"error\"]\n",
        "    }\n",
        "\n",
        "    logger.info(f\"text2sql_anthropic completed. Result: {json.dumps(result, default=str)[:200]}...\")\n",
        "    return result\n",
        "\n",
        "# Evaluation functions\n",
        "@evaluator()\n",
        "def no_error_evaluator(outputs, inputs, ground_truths=None):\n",
        "    \"\"\"Evaluate if the SQL query executed without errors\"\"\"\n",
        "    logger.info(f\"no_error_evaluator called with outputs type: {type(outputs)}\")\n",
        "\n",
        "    # Default values for invalid outputs\n",
        "    if not outputs or not isinstance(outputs, dict):\n",
        "        return {\"score\": 0.0, \"explanation\": \"Invalid output format\"}\n",
        "\n",
        "    # Check for error\n",
        "    error = outputs.get(\"error\")\n",
        "    model = outputs.get(\"model\", \"unknown\")\n",
        "    score = 1.0 if error is None else 0.0\n",
        "    explanation = f\"[{model}] Query executed successfully\" if score == 1.0 else f\"[{model}] Query execution failed: {error}\"\n",
        "\n",
        "    return {\"score\": score, \"explanation\": explanation}\n",
        "\n",
        "@evaluator()\n",
        "def has_results_evaluator(outputs, inputs, ground_truths=None):\n",
        "    \"\"\"Evaluate if the query returned results\"\"\"\n",
        "    logger.info(f\"has_results_evaluator called with outputs type: {type(outputs)}\")\n",
        "\n",
        "    # Default values for invalid outputs\n",
        "    if not outputs or not isinstance(outputs, dict):\n",
        "        return {\"score\": 0.0, \"explanation\": \"Invalid output format\"}\n",
        "\n",
        "    # Check for results\n",
        "    results = outputs.get(\"results\", [])\n",
        "    model = outputs.get(\"model\", \"unknown\")\n",
        "    has_results = results is not None and len(results) > 0\n",
        "    score = 1.0 if has_results else 0.0\n",
        "    explanation = f\"[{model}] Query returned {len(results) if results else 0} results\" if has_results else f\"[{model}] Query returned no results\"\n",
        "\n",
        "    return {\"score\": score, \"explanation\": explanation}\n",
        "\n",
        "@evaluator()\n",
        "def is_valid_sql_evaluator(outputs, inputs, ground_truths=None):\n",
        "    \"\"\"Evaluate if the generated SQL is valid using LLM\"\"\"\n",
        "    logger.info(f\"is_valid_sql_evaluator called with outputs type: {type(outputs)}\")\n",
        "\n",
        "    # Default values for invalid outputs\n",
        "    if not outputs or not isinstance(outputs, dict):\n",
        "        return {\"score\": 0.0, \"explanation\": \"Invalid output format\"}\n",
        "\n",
        "    # Get query\n",
        "    query = outputs.get(\"query\", \"\")\n",
        "    model = outputs.get(\"model\", \"unknown\")\n",
        "    if not query:\n",
        "        return {\"score\": 0.0, \"explanation\": f\"[{model}] No SQL query was generated\"}\n",
        "\n",
        "    IS_SQL_EVAL_TEMPLATE = \"\"\"You are a SQL expert, is the following a valid SQL query that executes without errors? Return the single word \"valid\" if it is valid, and \"invalid\" if it is not.\n",
        "\n",
        "    [BEGIN SQL QUERY]\n",
        "    {0}\n",
        "    [END SQL QUERY]\n",
        "    \"\"\"\n",
        "\n",
        "    response = completion(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a SQL expert evaluating SQL queries.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": IS_SQL_EVAL_TEMPLATE.format(query)\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    result = response.choices[0].message.content.strip().lower()\n",
        "    is_valid = \"valid\" in result\n",
        "    score = 1.0 if is_valid else 0.0\n",
        "    explanation = f\"[{model}] SQL query is {'valid' if is_valid else 'invalid'}\"\n",
        "\n",
        "    return {\"score\": score, \"explanation\": explanation}\n",
        "\n",
        "# For individual runs without tracing\n",
        "def run_individual_evaluations():\n",
        "    # Sample questions\n",
        "    questions = [\n",
        "        \"Which team won the most games?\",\n",
        "        \"Which team won the most games in 2015?\",\n",
        "        \"Who led the league in 3 point shots?\",\n",
        "        \"Which team had the biggest difference in records across two consecutive years?\",\n",
        "        \"What is the average number of free throws per year?\",\n",
        "    ]\n",
        "\n",
        "    logger.info(\"Starting individual evaluations\")\n",
        "\n",
        "    # Process each question with all models\n",
        "    for i, question in enumerate(questions):\n",
        "        # Create proper input format for the function\n",
        "        input_data = {\"question\": question}\n",
        "\n",
        "        # Process with OpenAI\n",
        "        result_openai = text2sql_openai(input_data)\n",
        "\n",
        "        # Process with Gemini\n",
        "        result_gemini = text2sql_gemini(input_data)\n",
        "\n",
        "        # Process with Anthropic\n",
        "        result_anthropic = text2sql_anthropic(input_data)\n",
        "\n",
        "        # Log results\n",
        "        print(f\"Question: {question}\")\n",
        "        print(\"\\n--- OpenAI (GPT-4o) Results ---\")\n",
        "        print(f\"Query: {result_openai['query']}\")\n",
        "        print(f\"Error: {result_openai['error']}\")\n",
        "        if result_openai['results']:\n",
        "            print(f\"Results: {result_openai['results'][:2]}...\")  # Show first 2 results\n",
        "        else:\n",
        "            print(\"No results\")\n",
        "\n",
        "        print(\"\\n--- Gemini Results ---\")\n",
        "        print(f\"Query: {result_gemini['query']}\")\n",
        "        print(f\"Error: {result_gemini['error']}\")\n",
        "        if result_gemini['results']:\n",
        "            print(f\"Results: {result_gemini['results'][:2]}...\")  # Show first 2 results\n",
        "        else:\n",
        "            print(\"No results\")\n",
        "\n",
        "        print(\"\\n--- Anthropic (Claude) Results ---\")\n",
        "        print(f\"Query: {result_anthropic['query']}\")\n",
        "        print(f\"Error: {result_anthropic['error']}\")\n",
        "        if result_anthropic['results']:\n",
        "            print(f\"Results: {result_anthropic['results'][:2]}...\")  # Show first 2 results\n",
        "        else:\n",
        "            print(\"No results\")\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Test case {i} complete\")\n",
        "\n",
        "    logger.info(\"Individual evaluations completed\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Script started\")\n",
        "\n",
        "    # Run individual evaluations without tracing\n",
        "    run_individual_evaluations()\n",
        "\n",
        "    # For batch evaluation with the evaluate function\n",
        "    # Create dataset with proper format following HoneyHive's expected structure\n",
        "    questions = [\n",
        "        \"Which team won the most games?\",\n",
        "        \"Which team won the most games in 2015?\",\n",
        "        \"Who led the league in 3 point shots?\",\n",
        "        \"Which team had the biggest difference in records across two consecutive years?\",\n",
        "        \"What is the average number of free throws per year?\",\n",
        "    ]\n",
        "\n",
        "    # Create dataset in the EXACT format expected by HoneyHive\n",
        "    dataset = [\n",
        "        {\n",
        "            \"inputs\": {\"question\": q},\n",
        "            \"ground_truths\": {}  # Optional, can be empty\n",
        "        }\n",
        "        for q in questions\n",
        "    ]\n",
        "\n",
        "    logger.info(f\"Created dataset for evaluation with proper structure: {dataset}\")\n",
        "\n",
        "    # Run evaluation with all models\n",
        "    logger.info(\"Starting batch evaluation with OpenAI\")\n",
        "    evaluate(\n",
        "        function=text2sql_openai,\n",
        "        hh_api_key=\"your honeyhive api key\",\n",
        "        hh_project=\"your honeyhive project name\",\n",
        "        name=\"your evaluation name\",\n",
        "        dataset=dataset,\n",
        "        evaluators=[\n",
        "            no_error_evaluator,\n",
        "            has_results_evaluator,\n",
        "            is_valid_sql_evaluator\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    logger.info(\"Starting batch evaluation with Gemini\")\n",
        "    evaluate(\n",
        "        function=text2sql_gemini,\n",
        "        hh_api_key=\"your honeyhive api key\",\n",
        "        hh_project=\"your honeyhive project name\",\n",
        "        name=\"your evaluation name\",\n",
        "        dataset=dataset,\n",
        "        evaluators=[\n",
        "            no_error_evaluator,\n",
        "            has_results_evaluator,\n",
        "            is_valid_sql_evaluator\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    logger.info(\"Starting batch evaluation with Anthropic\")\n",
        "    evaluate(\n",
        "        function=text2sql_anthropic,\n",
        "        hh_api_key=\"your honeyhive api key\",\n",
        "        hh_project=\"your honeyhive project name\",\n",
        "        name=\"name of your evaluation\",\n",
        "        dataset=dataset,\n",
        "        evaluators=[\n",
        "            no_error_evaluator,\n",
        "            has_results_evaluator,\n",
        "            is_valid_sql_evaluator\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    logger.info(\"All Text2SQL evaluations completed and pushed to HoneyHive\")\n",
        "    print(\"All Text2SQL evaluations completed and pushed to HoneyHive.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
